{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de Sentimentos\n",
    "\n",
    "### Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importações\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Created At</th>\n",
       "      <th>Text</th>\n",
       "      <th>Geo Coordinates.latitude</th>\n",
       "      <th>Geo Coordinates.longitude</th>\n",
       "      <th>User Location</th>\n",
       "      <th>Username</th>\n",
       "      <th>User Screen Name</th>\n",
       "      <th>Retweet Count</th>\n",
       "      <th>Classificacao</th>\n",
       "      <th>...</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "      <th>Unnamed: 18</th>\n",
       "      <th>Unnamed: 19</th>\n",
       "      <th>Unnamed: 20</th>\n",
       "      <th>Unnamed: 21</th>\n",
       "      <th>Unnamed: 22</th>\n",
       "      <th>Unnamed: 23</th>\n",
       "      <th>Unnamed: 24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Sun Jan 08 01:22:05 +0000 2017</td>\n",
       "      <td>���⛪ @ Catedral de Santo Antônio - Governador ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brasil</td>\n",
       "      <td>Leonardo C Schneider</td>\n",
       "      <td>LeoCSchneider</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun Jan 08 01:49:01 +0000 2017</td>\n",
       "      <td>� @ Governador Valadares, Minas Gerais https:/...</td>\n",
       "      <td>-41.9333</td>\n",
       "      <td>-18.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wândell</td>\n",
       "      <td>klefnews</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Sun Jan 08 01:01:46 +0000 2017</td>\n",
       "      <td>�� @ Governador Valadares, Minas Gerais https:...</td>\n",
       "      <td>-41.9333</td>\n",
       "      <td>-18.85</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wândell</td>\n",
       "      <td>klefnews</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Wed Jan 04 21:43:51 +0000 2017</td>\n",
       "      <td>��� https://t.co/BnDsO34qK0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ana estudando</td>\n",
       "      <td>estudandoconcur</td>\n",
       "      <td>0</td>\n",
       "      <td>Neutro</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>Mon Jan 09 15:08:21 +0000 2017</td>\n",
       "      <td>��� PSOL vai questionar aumento de vereadores ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Emily</td>\n",
       "      <td>Milly777</td>\n",
       "      <td>0</td>\n",
       "      <td>Negativo</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8194</td>\n",
       "      <td>8194</td>\n",
       "      <td>Thu Feb 09 11:48:07 +0000 2017</td>\n",
       "      <td>Trio é preso suspeito de roubo, tráfico e abus...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Ana Lúcia</td>\n",
       "      <td>lapiseirapentel</td>\n",
       "      <td>0</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8195</td>\n",
       "      <td>8195</td>\n",
       "      <td>Thu Feb 09 12:10:19 +0000 2017</td>\n",
       "      <td>Trio é preso suspeito de roubo, tráfico e abus...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Belo Horizonte - Minas Gerais</td>\n",
       "      <td>Marcelo Rezende</td>\n",
       "      <td>Televans</td>\n",
       "      <td>0</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8196</td>\n",
       "      <td>8196</td>\n",
       "      <td>Thu Feb 09 12:04:17 +0000 2017</td>\n",
       "      <td>Trio é preso suspeito de roubo, tráfico e abus...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Guarulhos - SP</td>\n",
       "      <td>Leonardo Nascimento</td>\n",
       "      <td>leonardogru</td>\n",
       "      <td>0</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8197</td>\n",
       "      <td>8197</td>\n",
       "      <td>Thu Feb 09 12:10:04 +0000 2017</td>\n",
       "      <td>Trio é preso suspeito de roubo, tráfico e abus...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Brasil Natal/RN</td>\n",
       "      <td>Lucas Medeiros �©™</td>\n",
       "      <td>parabolicalucas</td>\n",
       "      <td>0</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8198</td>\n",
       "      <td>8198</td>\n",
       "      <td>Tue Feb 07 15:01:30 +0000 2017</td>\n",
       "      <td>Trio suspeito de roubo de cargas é preso em Sa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Belo Horizonte - MG</td>\n",
       "      <td>RecordTV Minas</td>\n",
       "      <td>tvrecordminas</td>\n",
       "      <td>0</td>\n",
       "      <td>Positivo</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8199 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                      Created At  \\\n",
       "0              0  Sun Jan 08 01:22:05 +0000 2017   \n",
       "1              1  Sun Jan 08 01:49:01 +0000 2017   \n",
       "2              2  Sun Jan 08 01:01:46 +0000 2017   \n",
       "3              3  Wed Jan 04 21:43:51 +0000 2017   \n",
       "4              4  Mon Jan 09 15:08:21 +0000 2017   \n",
       "...          ...                             ...   \n",
       "8194        8194  Thu Feb 09 11:48:07 +0000 2017   \n",
       "8195        8195  Thu Feb 09 12:10:19 +0000 2017   \n",
       "8196        8196  Thu Feb 09 12:04:17 +0000 2017   \n",
       "8197        8197  Thu Feb 09 12:10:04 +0000 2017   \n",
       "8198        8198  Tue Feb 07 15:01:30 +0000 2017   \n",
       "\n",
       "                                                   Text  \\\n",
       "0     ���⛪ @ Catedral de Santo Antônio - Governador ...   \n",
       "1     � @ Governador Valadares, Minas Gerais https:/...   \n",
       "2     �� @ Governador Valadares, Minas Gerais https:...   \n",
       "3                           ��� https://t.co/BnDsO34qK0   \n",
       "4     ��� PSOL vai questionar aumento de vereadores ...   \n",
       "...                                                 ...   \n",
       "8194  Trio é preso suspeito de roubo, tráfico e abus...   \n",
       "8195  Trio é preso suspeito de roubo, tráfico e abus...   \n",
       "8196  Trio é preso suspeito de roubo, tráfico e abus...   \n",
       "8197  Trio é preso suspeito de roubo, tráfico e abus...   \n",
       "8198  Trio suspeito de roubo de cargas é preso em Sa...   \n",
       "\n",
       "      Geo Coordinates.latitude  Geo Coordinates.longitude  \\\n",
       "0                          NaN                        NaN   \n",
       "1                     -41.9333                     -18.85   \n",
       "2                     -41.9333                     -18.85   \n",
       "3                          NaN                        NaN   \n",
       "4                          NaN                        NaN   \n",
       "...                        ...                        ...   \n",
       "8194                       NaN                        NaN   \n",
       "8195                       NaN                        NaN   \n",
       "8196                       NaN                        NaN   \n",
       "8197                       NaN                        NaN   \n",
       "8198                       NaN                        NaN   \n",
       "\n",
       "                      User Location              Username User Screen Name  \\\n",
       "0                            Brasil  Leonardo C Schneider    LeoCSchneider   \n",
       "1                               NaN               Wândell         klefnews   \n",
       "2                               NaN               Wândell         klefnews   \n",
       "3                               NaN         Ana estudando  estudandoconcur   \n",
       "4                               NaN                 Emily         Milly777   \n",
       "...                             ...                   ...              ...   \n",
       "8194                            NaN             Ana Lúcia  lapiseirapentel   \n",
       "8195  Belo Horizonte - Minas Gerais       Marcelo Rezende         Televans   \n",
       "8196                 Guarulhos - SP   Leonardo Nascimento      leonardogru   \n",
       "8197                Brasil Natal/RN    Lucas Medeiros �©™  parabolicalucas   \n",
       "8198            Belo Horizonte - MG        RecordTV Minas    tvrecordminas   \n",
       "\n",
       "      Retweet Count Classificacao  ... Unnamed: 15  Unnamed: 16  Unnamed: 17  \\\n",
       "0                 0        Neutro  ...         NaN          NaN          NaN   \n",
       "1                 0        Neutro  ...         NaN          NaN          NaN   \n",
       "2                 0        Neutro  ...         NaN          NaN          NaN   \n",
       "3                 0        Neutro  ...         NaN          NaN          NaN   \n",
       "4                 0      Negativo  ...         NaN          NaN          NaN   \n",
       "...             ...           ...  ...         ...          ...          ...   \n",
       "8194              0      Positivo  ...         NaN          NaN          NaN   \n",
       "8195              0      Positivo  ...         NaN          NaN          NaN   \n",
       "8196              0      Positivo  ...         NaN          NaN          NaN   \n",
       "8197              0      Positivo  ...         NaN          NaN          NaN   \n",
       "8198              0      Positivo  ...         NaN          NaN          NaN   \n",
       "\n",
       "      Unnamed: 18  Unnamed: 19  Unnamed: 20  Unnamed: 21  Unnamed: 22  \\\n",
       "0             NaN          NaN          NaN          NaN          NaN   \n",
       "1             NaN          NaN          NaN          NaN          NaN   \n",
       "2             NaN          NaN          NaN          NaN          NaN   \n",
       "3             NaN          NaN          NaN          NaN          NaN   \n",
       "4             NaN          NaN          NaN          NaN          NaN   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "8194          NaN          NaN          NaN          NaN          NaN   \n",
       "8195          NaN          NaN          NaN          NaN          NaN   \n",
       "8196          NaN          NaN          NaN          NaN          NaN   \n",
       "8197          NaN          NaN          NaN          NaN          NaN   \n",
       "8198          NaN          NaN          NaN          NaN          NaN   \n",
       "\n",
       "      Unnamed: 23  Unnamed: 24  \n",
       "0             NaN          NaN  \n",
       "1             NaN          NaN  \n",
       "2             NaN          NaN  \n",
       "3             NaN          NaN  \n",
       "4             NaN          NaN  \n",
       "...           ...          ...  \n",
       "8194          NaN          NaN  \n",
       "8195          NaN          NaN  \n",
       "8196          NaN          NaN  \n",
       "8197          NaN          NaN  \n",
       "8198          NaN          NaN  \n",
       "\n",
       "[8199 rows x 26 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Abertura da base de dados\n",
    "\n",
    "df = pd.read_csv('data/Tweets_Mg.csv', sep=',')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pré-Processamento dos Dados\n",
    "\n",
    "#### 1) Checar se as classes estão balanceadas\n",
    "#### 2) Checar se há textos duplicados. Como se trata de Tweets, é bem possível\n",
    "#### 3) Remoção de Stopwords do idioma relacionado aos textos\n",
    "#### 4) Reduzir o vocabulário levando as palavras para seu radical\n",
    "####        4.1) Stemming ou Lemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Positivo    3300\n",
       "Neutro      2453\n",
       "Negativo    2446\n",
       "Name: Classificacao, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checando o balanceamento das classes\n",
    "#Estão mais ou menos parecidas\n",
    "\n",
    "df['Classificacao'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Como se trata de Tweets, há muitos casos em que o tweet é repetido, apenas mudando mínimas coisas ou até não mudando nada.\n",
    "#Como é o caso do retweet.\n",
    "#Dessa forma, é preciso realizar a checagem de registros duplicados\n",
    "\n",
    "df.drop_duplicates(['Text'], inplace=True) #É possível passar uma coluna específica para realizar a checagem de duplicados nos parâmetros\n",
    "\n",
    "\n",
    "\n",
    "#Antes 8199\n",
    "#Depois 5765"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5765"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Classificacao.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Neutro\n",
       "1         Neutro\n",
       "2         Neutro\n",
       "3         Neutro\n",
       "4       Negativo\n",
       "          ...   \n",
       "8194    Positivo\n",
       "8195    Positivo\n",
       "8196    Positivo\n",
       "8197    Positivo\n",
       "8198    Positivo\n",
       "Name: Classificacao, Length: 5765, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Como se trata de um problema de análise de sentimentos, os únicos atributos que vão ser necessários, é o que contém o texto\n",
    "#e o atributo que contém a rotulação. Todos os outros não agregam valor nesse contexto.\n",
    "\n",
    "#Separação do que importa\n",
    "\n",
    "textos = df['Text']\n",
    "\n",
    "sentimento = df['Classificacao']\n",
    "sentimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\preda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package rslp to\n",
      "[nltk_data]     C:\\Users\\preda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package rslp is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\preda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\preda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importação do ToolKit de PNL. NLTK\n",
    "#Download dos pacotes que vamos utilizar\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('rslp')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funções de Pré Processamento de Dados\n",
    "\n",
    "### 1) Remover Stopwords.\n",
    "##### - Carrega Cria a lista das Stopwords da lingua portuguesa\n",
    "##### - Faz a separação das palavras da string e itera sobre ela, checando se cada palavra está dentro da lista de stopwords\n",
    "##### - Retorna a junção de todas as palavras em uma string só com o join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'ao',\n",
       " 'aos',\n",
       " 'aquela',\n",
       " 'aquelas',\n",
       " 'aquele',\n",
       " 'aqueles',\n",
       " 'aquilo',\n",
       " 'as',\n",
       " 'até',\n",
       " 'com',\n",
       " 'como',\n",
       " 'da',\n",
       " 'das',\n",
       " 'de',\n",
       " 'dela',\n",
       " 'delas',\n",
       " 'dele',\n",
       " 'deles',\n",
       " 'depois',\n",
       " 'do',\n",
       " 'dos',\n",
       " 'e',\n",
       " 'ela',\n",
       " 'elas',\n",
       " 'ele',\n",
       " 'eles',\n",
       " 'em',\n",
       " 'entre',\n",
       " 'era',\n",
       " 'eram',\n",
       " 'essa',\n",
       " 'essas',\n",
       " 'esse',\n",
       " 'esses',\n",
       " 'esta',\n",
       " 'estamos',\n",
       " 'estas',\n",
       " 'estava',\n",
       " 'estavam',\n",
       " 'este',\n",
       " 'esteja',\n",
       " 'estejam',\n",
       " 'estejamos',\n",
       " 'estes',\n",
       " 'esteve',\n",
       " 'estive',\n",
       " 'estivemos',\n",
       " 'estiver',\n",
       " 'estivera',\n",
       " 'estiveram',\n",
       " 'estiverem',\n",
       " 'estivermos',\n",
       " 'estivesse',\n",
       " 'estivessem',\n",
       " 'estivéramos',\n",
       " 'estivéssemos',\n",
       " 'estou',\n",
       " 'está',\n",
       " 'estávamos',\n",
       " 'estão',\n",
       " 'eu',\n",
       " 'foi',\n",
       " 'fomos',\n",
       " 'for',\n",
       " 'fora',\n",
       " 'foram',\n",
       " 'forem',\n",
       " 'formos',\n",
       " 'fosse',\n",
       " 'fossem',\n",
       " 'fui',\n",
       " 'fôramos',\n",
       " 'fôssemos',\n",
       " 'haja',\n",
       " 'hajam',\n",
       " 'hajamos',\n",
       " 'havemos',\n",
       " 'hei',\n",
       " 'houve',\n",
       " 'houvemos',\n",
       " 'houver',\n",
       " 'houvera',\n",
       " 'houveram',\n",
       " 'houverei',\n",
       " 'houverem',\n",
       " 'houveremos',\n",
       " 'houveria',\n",
       " 'houveriam',\n",
       " 'houvermos',\n",
       " 'houverá',\n",
       " 'houverão',\n",
       " 'houveríamos',\n",
       " 'houvesse',\n",
       " 'houvessem',\n",
       " 'houvéramos',\n",
       " 'houvéssemos',\n",
       " 'há',\n",
       " 'hão',\n",
       " 'isso',\n",
       " 'isto',\n",
       " 'já',\n",
       " 'lhe',\n",
       " 'lhes',\n",
       " 'mais',\n",
       " 'mas',\n",
       " 'me',\n",
       " 'mesmo',\n",
       " 'meu',\n",
       " 'meus',\n",
       " 'minha',\n",
       " 'minhas',\n",
       " 'muito',\n",
       " 'na',\n",
       " 'nas',\n",
       " 'nem',\n",
       " 'no',\n",
       " 'nos',\n",
       " 'nossa',\n",
       " 'nossas',\n",
       " 'nosso',\n",
       " 'nossos',\n",
       " 'num',\n",
       " 'numa',\n",
       " 'não',\n",
       " 'nós',\n",
       " 'o',\n",
       " 'os',\n",
       " 'ou',\n",
       " 'para',\n",
       " 'pela',\n",
       " 'pelas',\n",
       " 'pelo',\n",
       " 'pelos',\n",
       " 'por',\n",
       " 'qual',\n",
       " 'quando',\n",
       " 'que',\n",
       " 'quem',\n",
       " 'se',\n",
       " 'seja',\n",
       " 'sejam',\n",
       " 'sejamos',\n",
       " 'sem',\n",
       " 'serei',\n",
       " 'seremos',\n",
       " 'seria',\n",
       " 'seriam',\n",
       " 'será',\n",
       " 'serão',\n",
       " 'seríamos',\n",
       " 'seu',\n",
       " 'seus',\n",
       " 'somos',\n",
       " 'sou',\n",
       " 'sua',\n",
       " 'suas',\n",
       " 'são',\n",
       " 'só',\n",
       " 'também',\n",
       " 'te',\n",
       " 'tem',\n",
       " 'temos',\n",
       " 'tenha',\n",
       " 'tenham',\n",
       " 'tenhamos',\n",
       " 'tenho',\n",
       " 'terei',\n",
       " 'teremos',\n",
       " 'teria',\n",
       " 'teriam',\n",
       " 'terá',\n",
       " 'terão',\n",
       " 'teríamos',\n",
       " 'teu',\n",
       " 'teus',\n",
       " 'teve',\n",
       " 'tinha',\n",
       " 'tinham',\n",
       " 'tive',\n",
       " 'tivemos',\n",
       " 'tiver',\n",
       " 'tivera',\n",
       " 'tiveram',\n",
       " 'tiverem',\n",
       " 'tivermos',\n",
       " 'tivesse',\n",
       " 'tivessem',\n",
       " 'tivéramos',\n",
       " 'tivéssemos',\n",
       " 'tu',\n",
       " 'tua',\n",
       " 'tuas',\n",
       " 'tém',\n",
       " 'tínhamos',\n",
       " 'um',\n",
       " 'uma',\n",
       " 'você',\n",
       " 'vocês',\n",
       " 'vos',\n",
       " 'à',\n",
       " 'às',\n",
       " 'é',\n",
       " 'éramos'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords = set(nltk.corpus.stopwords.words('portuguese'))\n",
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopwords(texto):\n",
    "    \n",
    "    stopwords = set(nltk.corpus.stopwords.words('portuguese'))\n",
    "    vocabulario = [i for i in texto.split() if not i in stopwords]\n",
    "    \n",
    "    return (\" \").join(vocabulario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oi tudo bem voce vai noite?'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = 'oi tudo bem como voce vai a noite?'\n",
    "mystring = 'https://oi.com.br'\n",
    "\n",
    "string1 = removeStopwords(string) #Removeu o 'a' pois considerou como uma stopwords da lingua portuguesa\n",
    "string1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de Stemming\n",
    "\n",
    "#### Reduz a palavra ao seu radical (prefixo) com o objetivo de diminuir o vocabulário de um texto, pois podem levar palavras diferentes ao mesmo prefixo. Porém, no Stemming, a redução ocorre e o resultado dela, não necessariamente representa uma palavra existente na língua.\n",
    "\n",
    "#### 1) Instancia o Objeto do Stemmer do nltk\n",
    "#### 2) Itera sobre as palavras de uma string separadas pelo split()\n",
    "#### 3) Aplica a função stem() do objeto criado, fazendo o append na lista de vocabulario\n",
    "#### 4) Para não retornar uma lista, faz-se o join() e junta tudo em uma única string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Stemming(texto):\n",
    "    \n",
    "    stemmer = nltk.stem.RSLPStemmer()\n",
    "    vocabulario = []\n",
    "    for i in texto.split():\n",
    "        vocabulario.append(stemmer.stem(i))\n",
    "    \n",
    "    return (\" \".join(vocabulario))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'oi tud bem com voc vai a noite?'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string2 = Stemming(string) #Removeu muitas coisas deixando so o radical\n",
    "string2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função para Limpeza dos Textos\n",
    "\n",
    "#### Remove pontuações, links, coisas que não agregam valor na tarefa de análise de sentimentos.\n",
    "#### Será feiro a partir de expressões regulares a substituição desses itens irregulares por espaço"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def limpar_textos(texto):\n",
    "    \n",
    "    vocabulario = re.sub(r\"http\\S\", \"\", texto).lower().replace('.', ' ').replace('-', ' ').replace(';', ' ').replace(':', ' ').replace('/','')\n",
    "    return (vocabulario)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' oi com br'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string3 = limpar_textos(mystring)\n",
    "string3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função Lemmatization\n",
    "\n",
    "#### Também atua como um redutor de palavras. Porém, diferente do Stemmer o Lemming reduz as palavras para um radical que reflete a realidade, ou seja, todas as palavras que o lemming reduz existem de forma concreta no dicionário. Dessa forma, possuem o mesmo objetivo, de reduzir o vocabulário.\n",
    "#### O passo a passo é semelhante\n",
    "\n",
    "####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Depende muito do idioma, não tem lemmatizer no wordnet para portugues\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def Lemmatization(texto):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    vocabulario = []\n",
    "    \n",
    "    for i in texto.split():\n",
    "        vocabulario.append(lemmatizer.lemmatize(i))\n",
    "        \n",
    "        return (\" \".join(vocabulario))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Os'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string4 = Lemmatization(\"Os carros são bonitos\")\n",
    "string4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função de PreProcessamento Completo \n",
    "\n",
    "#### Inclui todos os passos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessing(texto):\n",
    "    #Cria objeto stemming\n",
    "    stemmer = nltk.stem.RSLPStemmer()\n",
    "    \n",
    "    #Faz a limpeza com expressão regular substituindo\n",
    "    textoLimpo = re.sub(r\"http\\S\", \"\", texto).lower().replace('.', ' ').replace('-', ' ').replace(';', ' ').replace(':', ' ').replace('/','')\n",
    "    \n",
    "    #lista de stopwords\n",
    "    stopwords = set(nltk.corpus.stopwords.words('portuguese'))\n",
    "    \n",
    "    #aplicando o stemmer a cada elemento do texto\n",
    "    vocabulario = [stemmer.stem(i) for i in textoLimpo.split() if not i in stopwords]\n",
    "    \n",
    "    return (\" \".join(vocabulario))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preProcessing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-8be1efeb3f3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Textos pré- processados\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtextos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpreProcessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtextos\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtextos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-21-8be1efeb3f3f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Textos pré- processados\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mtextos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpreProcessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtextos\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mtextos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'preProcessing' is not defined"
     ]
    }
   ],
   "source": [
    "#Textos pré- processados\n",
    "\n",
    "textos = [preProcessing(i) for i in textos]\n",
    "textos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenização\n",
    "\n",
    "#### A maioria dos tokenizadores padrão apenas separam as palavras por espaço. Porém, cada caso pode ser diferente e no caso de Tweets, como é esse, há muito o uso de Hastags e emojis. Dessa forma, é melhor usar um tokenizador mais específico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Que', 'dia', 'lindo', '#', 'poderosa', ',', ';', ')']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tweet = \"Que dia lindo #poderosa, ;)\" #identifica o # separado da palavra e os emojis também, como sendo tokens separados\n",
    "\n",
    "word_tokenize(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Que', 'dia', 'lindo', '#poderosa', ',', ';)']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "#ANALISE DE TWITTER TEM QUE USAR ESSE TOKENIZADOR\n",
    "\n",
    "#Como em análise de sentimento, essas coisas são importantes, é bom q sejam identificadas juntas para análista\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "tokenizer.tokenize(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criando o Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5765, 13361)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vetorização do texto, estilo Bag of Words, criando o vocabulário e atribuindo um valor numerico para cada palavra que seria sua frequencia\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(analyzer=\"word\", tokenizer = tokenizer.tokenize) #passa o tokenizador que já foi criado\n",
    "\n",
    "freq_words = vectorizer.fit_transform(textos)\n",
    "freq_words.shape #matriz esparsa "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 3],\n",
       "       [0, 0, 0, ..., 0, 0, 1],\n",
       "       [0, 0, 0, ..., 0, 0, 2],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_words.A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "model = MultinomialNB() #Algoritmo Multinomial Naive Bayes\n",
    "model.fit(freq_words, sentimento)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Para fazer os testes, precisa aplicar nos dados de testes o mesmo passo a passo usado para treinar o modelo\n",
    "#### Nesse caso, só vetorizar mesmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizando testes com o modelo recém treinado\n",
    "\n",
    "testes = ['Eu não gostei disso', \n",
    "          'Eu gosto disso', \n",
    "          'Estou muito triste para falar disso!!!', \n",
    "          'Eu amo você']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 13361)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_testes = vectorizer.transform(testes) #Vectorizer ja está treinado com os tex\n",
    "freq_testes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Neutro', 'Neutro', 'Neutro', 'Neutro'], dtype='<U8')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = model.predict(freq_testes)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eu não gostei disso, Neutro\n",
      "Eu gosto disso, Neutro\n",
      "Estou muito triste para falar disso!!!, Neutro\n",
      "Eu amo você, Neutro\n"
     ]
    }
   ],
   "source": [
    "#Classificação\n",
    "\n",
    "for t, c in zip (testes, model.predict(freq_testes)):\n",
    "    print(t + \", \" + c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Negativo' 'Neutro' 'Positivo']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.1 , 0.9 , 0.01],\n",
       "       [0.12, 0.86, 0.02],\n",
       "       [0.01, 0.99, 0.  ],\n",
       "       [0.02, 0.98, 0.01]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Como está usando Naive Bayes, é possivel imporimir a probabilidade de ser de cada classe\n",
    "#É atribuido a classe que possuir a maior probabilidade\n",
    "\n",
    "print(model.classes_)\n",
    "model.predict_proba(freq_testes).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tags de Negação\n",
    "\n",
    "#### No modelo de Bag of Words, às vezes é complicado para fazer a diferenciação de duas frases iguais, porém uma contendo um \"não\", pois muda pouquíssima coisa.\n",
    "#### O Bag of Words não consegue fazer bem a distinção e saber que um \"não\" pode estar negativando uma frase inteira.\n",
    "#### Para minimizar isso, pode ser acrescentado uma tag de negação toda vez que o algoritmo encontrar um não na base de dados.\n",
    "#### Quando é encontrada um \"não\" , é colocada em todas as palavras seguintes uma tag de negação e quando treinar o modelo, dará um peso maior negativo àquela frase. Aumenta o vocabulário mas deve melhorar a precisão.\n",
    "\n",
    "#### Ajuda a classificar frases com inversão de sentimento com mais precisão\n",
    "#### Exemplo:\n",
    "#### - Eu gosto de comer carne\n",
    "#### - Eu não gosto de comer carne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função para Adicionar as Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_neg(texto):\n",
    "    tags_neg = ['não', 'not']\n",
    "    neg_detected = False\n",
    "    vocabulario_neg = []\n",
    "    vocabulario = texto.split()\n",
    "    \n",
    "    for i in vocabulario:\n",
    "        i = i.lower()\n",
    "        \n",
    "        if neg_detected == True:\n",
    "            i = i + '_NEG'\n",
    "        \n",
    "        if i in tags_neg:\n",
    "            neg_detected = True\n",
    "        \n",
    "        vocabulario_neg.append(i)\n",
    "    \n",
    "    return (\" \".join(vocabulario_neg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eu não gosto_NEG de_NEG queijo_NEG'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Teste com frase\n",
    "#Adicionou as tags, aumentando o vocabulario, porém, dando mais peso negativo para esta frase\n",
    "\n",
    "tag_neg(\"Eu não gosto de queijo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Criar modelos com Pipeline\n",
    "\n",
    "\n",
    "#### Pipeline é um objeto que encadeia tarefas.\n",
    "#### É interessante para reduzir código e automatizar fluxos\n",
    "#### Vantagem: pode-se criar varios pipelines mudando apenas alguns parâmetros e executar ao mesmo tempo para ver qual é o melhor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('count', CountVectorizer()),\n",
    "    ('classifier', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_neg = Pipeline([\n",
    "    ('count', CountVectorizer(tokenizer = lambda text: tag_neg(text))),\n",
    "    ('classifier', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('count',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('classifier',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#treinamento\n",
    "\n",
    "pipeline.fit(textos, sentimento) #pode-se ver os parãmetros para poder alterar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('count',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function <lambda> at 0x000001C7F6827828>,\n",
       "                                 vocabulary=None)),\n",
       "                ('classifier',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_neg.fit(textos,sentimento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('count',\n",
       "  CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                  dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                  lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                  ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                  strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                  tokenizer=<function <lambda> at 0x000001C7F6827828>,\n",
       "                  vocabulary=None)),\n",
       " ('classifier', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_neg.steps #etapas do pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM (Support Vector Machine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "pipeline_svm = Pipeline([\n",
    "    ('count', CountVectorizer()),\n",
    "    ('classifier', svm.SVC(kernel='linear'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_neg = Pipeline([\n",
    "    ('count', CountVectorizer(tokenizer = lambda text: tag_neg(text))),\n",
    "    ('classifier', svm.SVC(kernel='linear'))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validando os modelos com Validação Cruzada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validação cruzada com cv = 10\n",
    "\n",
    "resultado = cross_val_predict(pipeline, textos, sentimento, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8716392020815265"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Acurácia média\n",
    "\n",
    "metrics.accuracy_score(sentimento, resultado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Positivo       0.95      0.89      0.92      2840\n",
      "    Negativo       0.80      0.87      0.83       951\n",
      "      Neutro       0.81      0.84      0.83      1974\n",
      "\n",
      "    accuracy                           0.87      5765\n",
      "   macro avg       0.85      0.87      0.86      5765\n",
      "weighted avg       0.88      0.87      0.87      5765\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentimentos = ['Positivo', 'Negativo', 'Neutro']\n",
    "\n",
    "print(metrics.classification_report(sentimento, resultado, sentimentos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "resultado_neg = cross_val_predict(pipeline_neg,textos,sentimento,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7694709453599307"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Acurácia média\n",
    "\n",
    "metrics.accuracy_score(sentimento, resultado_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliando modelo com Brigrama\n",
    "\n",
    "#### bigramas são conjuntos de duas palavras. Cada token vai ser a junção de duas palavras\n",
    "#### Quanto maior o valor de n em ngrama, mais contexto é passado para o algoritmo, porém não fica tão preciso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_bigram = CountVectorizer(ngram_range=(2,2)) #Passando a range para usar 2 palavras por token\n",
    "freq_bigram = vectorize_bigram.fit_transform(textos)\n",
    "\n",
    "modelo = MultinomialNB()\n",
    "modelo.fit(freq_bigram,sentimento)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = cross_val_predict(modelo, freq_bigram, sentimento, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8412836079791848"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics.accuracy_score(sentimento, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Considerações Finais\n",
    "\n",
    "#### 1) Tentar outros features para agregar o modelo (foi usado apenas o texto puro com algumas transformações, não teve nenhum outro feature). Exemplo: tamanho do texto, emoticons.... etc\n",
    "#### 2) Se tiver uma base APENAS com duas classes (positiva e negativa) o SVM Linear tende a funcionar muito bem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
